# -*- coding: utf-8 -*-
import logging
from scrapy import signals
from scrapy.exceptions import NotConfigured
import json
import MySQLdb
from sfDataCrawl import settings
import math

logger = logging.getLogger(__name__)

class DataSyncExtension(object):
    product_db_conn = ''
    product_db_cursor = ''
    def __init__(self, ):
        self.product_db_conn = ''
        self.product_db_cursor = ''
    @classmethod
    def from_crawler(cls, crawler):
        # instantiate the extension object
        ext = cls()
        # connect the extension object to signals
        crawler.signals.connect(ext.spider_opened, signal=signals.spider_opened)
        crawler.signals.connect(ext.spider_closed, signal=signals.spider_closed)
        crawler.signals.connect(ext.item_scraped, signal=signals.item_scraped)

        # return the extension object
        return ext

    def spider_opened(self, spider):
        if spider.name == 'test':
            crawl_db_conn = spider.conn
            crawl_db_cursor = spider.cursor
            self.product_db_conn = MySQLdb.connect(settings.PRODUCT_MYSQL_HOST, settings.PRODUCT_MYSQL_USER,
                                                   settings.PRODUCT_MYSQL_PWD,settings.PRODUCT_MYSQL_DB,
                                                   charset='utf8',use_unicode=True)
            self.product_db_cursor = self.product_db_conn.cursor()
            # sync the data from product system to sf_crawl database
            if not settings.SKIP_IMPORTING:
                try:
                    self._import_company_data(self.product_db_cursor, crawl_db_conn, crawl_db_cursor)
                    self._import_manager_data(self.product_db_cursor, crawl_db_conn, crawl_db_cursor)
                    self._import_product_data(self.product_db_conn, self.product_db_cursor, crawl_db_conn, crawl_db_cursor)
                    self._import_manager_product_data(self.product_db_conn, self.product_db_cursor, crawl_db_conn, crawl_db_cursor)
                    self._import_hs_index(self.product_db_conn, self.product_db_cursor, crawl_db_conn, crawl_db_cursor)
                    self._import_product_nav(self.product_db_conn, self.product_db_cursor, crawl_db_conn, crawl_db_cursor)
                    self._import_return_darwdown(self.product_db_conn, self.product_db_cursor, crawl_db_conn, crawl_db_cursor)
                    self._import_statistics(self.product_db_conn, self.product_db_cursor, crawl_db_conn, crawl_db_cursor)
                except MySQLdb.Error,e:
                    spider.spider_log.info('import data Error' + str(e))
                    raise
                pass
            pass
        logger.info("opened spider %s", spider.name)

    def spider_closed(self, spider):
        if spider.name == 'simu':
            #sync the data from sf_crawl to the product system
            if not settings.SKIP_EXPORTING:
                pass
            #free the conn and cursor
            self.product_db_conn = ''
            self.product_db_cursor = ''
            pass
        logger.info("closed spider %s", spider.name)

    def item_scraped(self, item, spider):
        pass

    def _import_product_data(self, product_conn, product_cursor, crawl_conn, crawl_cursor):
        #获取crawl数据中最大的产品Id，将线上环境中大于该Id的insert进来，剩余更新进来
        product_items = ['id','name','full_name','start_date','end_date','init_nav','issuing_scale','trustee_bank',
                         'access_role','status','product_status','purchase_status','product_type','list_order',
                         'latest_nav','nav_date','added_nav','company_id','company_name','manager_list',
                         'return_recent_month1','return_recent_month3','return_recent_month6',
                         'return_recent_year1','return_recent_year2','return_recent_year3','return_recent_year4',
                         'return_recent_year5','return_year1','return_year2','return_year3','return_year4',
                         'return_year5','grade','nav_trustworthy','appraise','tag_type','min_purchase_amount',
                         'min_append_amount','structured','update_time','create_time']
        self._sync_table_data(product_cursor, crawl_cursor, crawl_conn, 'sf_product', items=product_items)
        #upsert the companydata part
        product_data_items = ['id', 'description', 'orientation', 'investment_idea', 'investment_restriction',
                              'investment_target', 'investment_strategy','investment_range','asset_allocation',
                              'comparison_datum', 'remark', 'subscription_fee', 'fixed_management_fee', 'ransom_fee',
                              'commission', 'attachment_url', 'raise_ratio', 'rebate_type', 'rebate', 'account_number',
                              'account_name', 'account_bank', 'account_remark','expected_return', 'expected_return_desc',
                              'open_date', 'locked_time','locked_time_desc', 'duration', 'redemption_status',
                              'nav_data_freq', 'precautious_line','precautious_line_desc', 'stop_loss_line',
                              'stop_loss_line_desc', 'expected_guaranteed_return','risk_income_character',
                              'income_distribution','create_time', 'update_time']
        self._sync_table_data(product_cursor, crawl_cursor, crawl_conn, 'sf_product_data', items=product_data_items)
    def _import_manager_data(self, product_cursor, crawl_conn, crawl_cursor):
        # 获取crawl数据中最大的经理Id，将线上环境中大于该Id的insert进来，剩余更新进来
        max_crawl_entity_id = ''
        max_crawl_update_time = ''
        max_product_entity_id = ''
        max_product_update_time = ''
        number_per_round = 1000
        insert_entities_list = list()
        insert_entities_data_list = list()
        max_crawl_sql = 'SELECT MAX(id) as maxId, MAX(update_time) as maxUpdateTime from sf_manager'
        max_crawl_info = self._get_max_info(crawl_cursor,max_crawl_sql)
        max_crawl_entity_id = max_crawl_info[0]
        max_crawl_update_time = max_crawl_info[1]
        max_product_sql = "SELECT MAX(id) as maxId, Max(update_time) as maxUpdateTime from sf_manager"
        product_max_info = self._get_max_info(product_cursor,max_product_sql)
        max_product_entity_id = product_max_info[0]
        max_product_update_time = product_max_info[1]
        if max_product_entity_id > max_crawl_entity_id:
            max_insert_sql = 'SELECT MAX(update_time) as maxUpdateTime from sf_manager WHERE id <= %s'
            max_insert_param = [max_crawl_entity_id]
            max_product_info = self._get_max_info(product_cursor, max_insert_sql,max_insert_param)
            max_product_update_time = max_product_info[0]
            page_sql = 'SELECT count(1) as toInsertNum from sf_manager where id > %s'
            page_param = [max_crawl_entity_id]
            insert_page = self._get_page_total_number(product_cursor, page_sql, page_param, number_per_round)
            for insert_index in range(0, insert_page):
                select_sql = 'SELECT id,name,name_en,nick_initial,sex,education,company_id, company_name, \
                profile, background, avatar, status, invest_year, list_order, appraise, typical_product_id, \
                manage_product_num, create_time, update_time FROM sf_manager WHERE id > %s ORDER BY id LIMIT %s'
                select_param = [max_crawl_entity_id, (insert_index + 1) * 1000]
                insert_sql = 'INSERT INTO sf_manager (id,name,name_en,nick_initial,sex,education,company_id, \
                  company_name,profile, background, avatar, status, invest_year, list_order, appraise, \
                  typical_product_id,manage_product_num, create_time, update_time) VALUES(%s,%s,%s,%s,%s,%s,%s,%s, \
                  %s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)'
                #insert manager table data
                self._insert_data(product_cursor,crawl_cursor,crawl_conn,select_sql,select_param,insert_sql)
        if max_product_update_time > max_crawl_update_time :
            page_sql = 'SELECT COUNT(1) from sf_manager where update_time > %s'
            page_param = [max_crawl_update_time]
            total_page = self._get_page_total_number(product_cursor,page_sql,page_param,number_per_round)
            for update_index in range(0,total_page):
                select_sql = 'SELECT name,name_en,nick_initial,sex,education,company_id, company_name, \
                                profile, background, avatar, status, invest_year, list_order, appraise, typical_product_id, \
                                manage_product_num, create_time, update_time,id FROM sf_manager WHERE update_time > %s ORDER BY id LIMIT %s'
                select_param = [max_crawl_update_time, (update_index + 1) * 1000]
                update_sql = 'UPDATE sf_manager SET name = %s, name_en = %s, nick_initial = %s, sex = %s, education = %s,\
                          company_id = %s, company_name = %s, profile = %s, background = %s, avatar = %s, status = %s, \
                          invest_year = %s, list_order = %s, appraise = %s, typical_product_id = %s, manage_product_num = %s,\
                          create_time = %s, update_time = %s WHERE id = %s'
                self._update_data_by_id(product_cursor,crawl_cursor,crawl_conn,select_sql,select_param,update_sql)
                pass
            pass
        pass
    def _import_company_data(self, product_cursor, crawl_conn, crawl_cursor):
        #获取crawl数据中最大的公司Id，将线上环境中大于该Id的insert进来，剩余更新进来
        #暂时使用company和company_data分开处理两张表，为了代码书写方便。如果后期性能受到影响，合并读取
        max_crawl_company_id = ''
        max_crawl_update_time = ''
        max_product_company_id = ''
        max_product_update_time = ''
        insert_per_round = 1000
        inserted_line_number = 0
        max_sql = 'SELECT MAX(id) as maxId, MAX(update_time) as maxUpdateTime  FROM sf_company'
        max_info = self._get_max_info(crawl_cursor,max_sql)
        max_crawl_company_id = max_info[0]
        max_crawl_update_time = max_info[1]
        product_max_sql = 'SELECT MAX(id) as maxId, MAX(update_time) as maxUpdateTime  FROM sf_company'
        product_max_info = self._get_max_info(product_cursor, product_max_sql)
        max_product_company_id = product_max_info[0]
        max_product_update_time = product_max_info[1]
        if max_product_company_id > max_crawl_company_id :
            max_insert_sql = 'SELECT MAX(update_time) as maxUpdateTime from sf_company WHERE id <= %s'
            max_param = [max_crawl_company_id]
            max_insert_info = self._get_max_info(product_cursor, max_insert_sql, max_param)
            max_product_update_time = max_insert_info[0]
            product_cursor.execute('SELECT count(1)  FROM sf_company WHERE id > %s',[max_crawl_company_id])
            inserted_number_info = product_cursor.fetchall()
            for temp_info in inserted_number_info:
                inserted_line_number = temp_info[0]
            temp_page_number = (inserted_line_number)/insert_per_round
            if divmod(inserted_line_number, insert_per_round) > 0 :
                page_number = int(math.floor(temp_page_number) + 1)
            else:
                page_number = int(temp_page_number)
            for i in range(0,page_number):
                product_cursor.execute('SELECT id, name, full_name, nick_initial, core_manager_id, \
                                     core_manager_name, rep_product, website_addr, icp, \
                                     establishment_date, registered_capital, country, \
                                     province, city, asset_mgmt_scale, logo, list_order, \
                                     status, product_count, create_time, update_time from sf_company WHERE id > %s  ORDER BY \
                                     id LIMIT %s', [max_crawl_company_id, (i+1)*1000]
                                       )
                companies = product_cursor.fetchall()
                inserted_company = list()
                for company in companies:
                    inserted_company.append(company)
                crawl_cursor.executemany('INSERT INTO sf_company (id, name, full_name, nick_initial, core_manager_id, \
                                         core_manager_name,rep_product, website_addr, icp, establishment_date, registered_capital, country, \
                                         province, city, asset_mgmt_scale, logo, list_order, status, product_count,  \
                                         create_time, update_time) VALUES (%s,%s,%s,%s,%s,%s,%s,%s,%s,%s, %s,\
                                         %s,%s,%s,%s,%s,%s,%s,%s,%s,%s)',inserted_company)
                crawl_conn.commit()
                product_cursor.execute('SELECT id, team, investment_idea, description, short_profile from sf_company_data \
                                  WHERE id > %s  ORDER BY id DESC LIMIT %s', [max_crawl_company_id, (i + 1) * 1000])
                companies_data = product_cursor.fetchall()
                inserted_company_data = list()
                for company in companies_data:
                    inserted_company_data.append(company)
                crawl_cursor.executemany('INSERT INTO sf_company_data (id, team, investment_idea, description, short_profile) \
                                VALUES (%s,%s,%s,%s,%s)',inserted_company_data)
                crawl_conn.commit()
        if max_product_update_time > max_crawl_update_time:
            to_update_number = 0
            product_cursor.execute('SELECT COUNT(1) as toUpdateNum FROM sf_company WHERE update_time > %s',[max_crawl_update_time])
            to_update_info = product_cursor.fetchall()
            for update_line in to_update_info:
                to_update_number = update_line[0]
            if divmod(to_update_number,insert_per_round)[1] > 0:
                to_update_total_page = int(math.floor(to_update_number/insert_per_round) + 1)
            else:
                to_update_total_page = int(math.floor(to_update_number/insert_per_round))
            for update_index in range(0,to_update_total_page):
                product_cursor.execute('SELECT id, name, full_name, nick_initial, core_manager_id, \
                                     core_manager_name, rep_product, website_addr, icp, \
                                     establishment_date, registered_capital, country, \
                                     province, city, asset_mgmt_scale, logo, list_order, \
                                     status, product_count, create_time, update_time from sf_company WHERE update_time > %s  ORDER BY \
                                     id LIMIT %s', [max_crawl_update_time, (update_index+1)*1000]
                                       )
                update_companies = product_cursor.fetchall()
                updated_companies = list()
                for update_company in update_companies:
                    updated_companies.append(update_company)
                for company in updated_companies:
                    #for now update the info one by one
                    company_id = company[0]
                    company = company + (company_id,)
                    crawl_cursor.execute('UPDATE sf_company SET name = %s, full_name = %s, nick_initial = %s,\
                                         core_manager_id = %s, core_manager_name = %s, rep_product = %s, website_addr = %s, icp = %s,\
                                         establishment_date = %s, registered_capital = %s, country = %s, \
                                         province = %s, city = %s, asset_mgmt_scale = %s, logo = %s, list_order = %s, \
                                         status = %s, product_count = %s, create_time = %s, update_time = %s WHERE id = %s',
                                         [company[1],company[2],company[3],company[4],company[5],company[6],company[7],
                                          company[8], company[9], company[10], company[11], company[12], company[13], company[14],
                                          company[15], company[16], company[17], company[18], company[19],company[20],company[0]
                                          ])
                crawl_conn.commit()
            pass
    def _import_manager_product_data(self, product_conn, product_cursor, crawl_conn, crawl_cursor):
        pass
    def _import_product_nav(self, product_conn, product_cursor, crawl_conn, crawl_cursor):
        pass
    def _import_hs_index(self, product_conn, product_cursor, crawl_conn, crawl_cursor):
        pass
    def _import_dividend_split(self, product_conn, product_cursor, crawl_conn, crawl_cursor):
        pass
    def _import_return_darwdown(self, product_conn, product_cursor, crawl_conn, crawl_cursor):
        pass
    def _import_statistics(self, product_conn, product_cursor, crawl_conn, crawl_cursor):
        pass

    def _export_product_data(self, product_conn, product_cursor, crawl_conn, crawl_cursor):
        #获取crawl数据中最大的产品Id，将线上环境中大于该Id的insert进来，剩余更新进来
        pass
    def _export_manager_data(self, product_conn, product_cursor, crawl_conn, crawl_cursor):
        # 获取crawl数据中最大的经理Id，将线上环境中大于该Id的insert进来，剩余更新进来
        pass
    def _export_company_data(self, product_conn, product_cursor, crawl_conn, crawl_cursor):
        pass
    def _export_manager_product_data(self, product_conn, product_cursor, crawl_conn, crawl_cursor):
        pass
    def _export_product_nav(self, product_conn, product_cursor, crawl_conn, crawl_cursor):
        pass
    def _export_hs_index(self, product_conn, product_cursor, crawl_conn, crawl_cursor):
        pass
    def _export_dividend_split(self, product_conn, product_cursor, crawl_conn, crawl_cursor):
        pass
    def _export_return_darwdown(self, product_conn, product_cursor, crawl_conn, crawl_cursor):
        pass
    def _export_statistics(self, product_conn, product_cursor, crawl_conn, crawl_cursor):
        pass

    def _move_first_to_last_for_tuple(self,tuple_data):
        if len(tuple_data) < 2:
            return tuple_data
        list_data = list()
        first_element = ''
        for i in range(len(tuple_data)):
            if i == 0:
                first_element = tuple_data[i]
            else:
                list_data.append(tuple_data[i])
        list_data.append(first_element)
        return_tuple = tuple(list_data)
        return return_tuple
    def _insert_data(self,source_cursor, target_cursor, target_conn, select_sql, select_param,insert_sql):
        insert_entities_list = list()
        source_cursor.execute(select_sql,select_param)
        insert_entities = source_cursor.fetchall()
        for insert_entity in insert_entities:
            insert_entities_list.append(insert_entity)
        target_cursor.executemany(insert_sql, insert_entities_list)
        target_conn.commit()
        pass
    def _update_data_by_id(self, source_cursor, target_cursor, target_conn, select_sql, select_param, update_sql):
        source_cursor.execute(select_sql,select_param)
        update_info = source_cursor.fetchall()
        for update_line in update_info:
            # update_tuple = self._move_first_to_last_for_tuple(update_line)
            target_cursor.execute(update_sql,list(update_line))
        target_conn.commit()
    def _get_page_total_number(self,source_cursor,sql,param,round_number = 1000):
        source_cursor.execute(sql,param)
        page_info = source_cursor.fetchall()
        for line in page_info:
            item_number = line[0]
        if divmod(item_number, round_number)[1] > 0:
            total_page_number = int(math.floor(divmod(item_number,round_number)[0]) + 1)
        else:
            total_page_number = int(math.floor(divmod(item_number, round_number)[0]))
        return total_page_number
    def _get_max_info(self,source_cursor,select_sql, select_param=None):
        if select_param is None:
            source_cursor.execute(select_sql)
        else:
            source_cursor.execute(select_sql,select_param)
        max_info = source_cursor.fetchall()
        for max_line in max_info:
            return max_line
    def _sync_table_data(self,source_cursor,target_cursor,target_conn,table_name,items=[]):
        number_per_round = 1000
        insert_entities_list = list()
        insert_entities_data_list = list()
        max_crawl_sql = 'SELECT MAX(id) as maxId, MAX(update_time) as maxUpdateTime from ' + table_name
        max_crawl_info = self._get_max_info(target_cursor, max_crawl_sql)
        max_crawl_entity_id = max_crawl_info[0]
        max_crawl_update_time = max_crawl_info[1]
        max_product_sql = 'SELECT MAX(id) as maxId, Max(update_time) as maxUpdateTime from ' + table_name
        product_max_info = self._get_max_info(source_cursor, max_product_sql)
        max_product_entity_id = product_max_info[0]
        max_product_update_time = product_max_info[1]
        if max_product_entity_id > max_crawl_entity_id:
            max_insert_sql = 'SELECT MAX(update_time) as maxUpdateTime from ' + table_name + ' WHERE id <= %s'
            max_insert_param = [max_crawl_entity_id]
            max_product_info = self._get_max_info(source_cursor, max_insert_sql, max_insert_param)
            max_product_update_time = max_product_info[0]
            page_sql = 'SELECT count(1) as toInsertNum from ' + table_name + ' where id > %s'
            page_param = [max_crawl_entity_id]
            insert_page = self._get_page_total_number(source_cursor, page_sql, page_param, number_per_round)
            for insert_index in range(0, insert_page):
                select_sql = 'SELECT '
                i_counter = 0
                for item_name in items:
                    if i_counter != 0:
                        select_sql = select_sql + ',' + item_name
                    else:
                        select_sql = select_sql + item_name
                    i_counter += 1
                select_sql = select_sql + ' FROM ' + table_name + ' WHERE id > %s ORDER BY id LIMIT %s'
                select_param = [max_crawl_entity_id, (insert_index + 1) * 1000]
                insert_sql = 'INSERT INTO  ' + table_name + '  ('
                i_counter = 0
                for item_name in items:
                    if i_counter != 0:
                        insert_sql = insert_sql + ',' + item_name
                    else:
                        insert_sql = insert_sql + item_name
                    i_counter += 1
                insert_sql = insert_sql + ') VALUES('
                item_length = len(items);
                for i in range(0, item_length):
                    if i != 0:
                        insert_sql = insert_sql + ',%s'
                    else:
                        insert_sql = insert_sql + '%s'
                insert_sql = insert_sql + ')'
                # insert manager table data
                print select_sql,'-----------',insert_sql
                self._insert_data(source_cursor, target_cursor, target_conn, select_sql, select_param, insert_sql)
        if max_product_update_time > max_crawl_update_time:
            page_sql = 'SELECT COUNT(1) from  ' + table_name + '  where update_time > %s'
            page_param = [max_crawl_update_time]
            total_page = self._get_page_total_number(source_cursor, page_sql, page_param, number_per_round)
            for update_index in range(0, total_page):
                select_sql = 'SELECT '
                item_index = 0
                for item_name in items:
                    if item_index != 0 :
                        if item_index == 1:
                            select_sql += item_name
                        else:
                            select_sql += ',' + item_name
                    item_index += 1
                select_sql = select_sql + ',' + items[0] + ' FROM  ' + table_name + ' WHERE update_time > %s ORDER BY id LIMIT %s'
                select_param = [max_crawl_update_time, (update_index + 1) * 1000]
                update_sql = 'UPDATE ' + table_name + ' SET '
                for i in range(0, len(items)):
                    if i > 0 :
                        if i != 1:
                            update_sql += ',' + items[i] + ' = %s'
                        else:
                            update_sql += items[i] + '=%s'
                update_sql += ' WHERE '+ items[0] + ' =%s'
                print select_sql, '000000000',update_sql
                self._update_data_by_id(source_cursor, target_cursor, target_conn, select_sql, select_param, update_sql)
                pass
            pass
        pass
        pass