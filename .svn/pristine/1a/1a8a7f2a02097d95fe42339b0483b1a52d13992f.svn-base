# -*- coding: utf-8 -*-

# Define your item pipelines here
#
# Don't forget to add your pipeline to the ITEM_PIPELINES setting
# See: http://doc.scrapy.org/en/latest/topics/item-pipeline.html
import MySQLdb
import datetime
import calendar
from sfDataCrawl import settings

def get_date_by_interval(source_date, interval_type=1):
    o_source_date =datetime.datetime.strptime(source_date, '%Y-%m-%d').date()
    source_year = o_source_date.year
    source_month = o_source_date.month
    source_date = o_source_date.day
    result_date = source_date
    target_year = ''
    target_month = ''
    target_day = ''
    if interval_type == 1 or interval_type == 2 or interval_type == 3:
        #近几月
        month_type = {1: 1, 2: 3, 3: 6}
        interval_value = month_type[interval_type]
        if source_month < interval_value :
            target_year = source_year - 1
        else:
            target_year = source_year
        target_month = divmod(source_month + 12 - interval_value, 12)[1]
        count_day = calendar.monthrange(target_year,target_month)[1]
        if count_day < source_date:
            target_day = count_day
        else:
            target_day = source_date
        result_date = datetime.date(target_year, target_month, target_day)
    elif interval_type == 4:
        #今年以来
        target_year = source_year
        result_date = datetime.date(target_year, 1, 1)
        pass
    elif interval_type == 5 or interval_type == 6 or interval_type == 7 or interval_type == 8 or interval_type == 9:
        #近几年
        year_type = {5 : 1, 6 : 2, 7 : 3, 8 : 4, 9 : 5}
        interval_value = year_type[interval_type]
        target_year = source_year - interval_value
        target_month = source_month
        count_day = calendar.monthrange(target_year, target_month)[1]
        if count_day < source_date:
            target_day = count_day
        else:
            target_day = source_date
        result_date = datetime.date(target_year, target_month, target_day)
        pass
    elif interval_type == 10 or interval_type == 11 or interval_type == 12 or interval_type == 13 or interval_type == 14:
        year_type = {10 : 0, 11 : 1, 12 : 2, 13 : 3, 14 : 4}
        interval_value = year_type[interval_type]
        target_year = source_year - interval_value
        target_month = 1
        target_day = 1
        result_date = datetime.date(target_year, target_month, target_day)
    return result_date

class SfDataCrawlPipeline(object):
    host = '192.168.83.31'
    user_name = 'root'
    password = 'Jrcs@15F'
    db_name = 'sf_crawl'
    cursor = ''
    conn = ''
    def __init__(self):
        try:
            self.conn = MySQLdb.connect(self.host, self.user_name, self.password, self.db_name, charset='utf8',use_unicode=True)
            self.cursor = self.conn.cursor()
        except MySQLdb.Error,e:
            print 'initial error'
            print str(e)
    def process_item(self, item, spider):
        #test db in the localhost
        # if 'name1' in item:
        #     try:
        #         self.cursor.execute('INSERT INTO test(name) VALUES(%s)', [item['name1']])
        #         self.conn.commit()
        #     except MySQLdb.Error, e:
        #         print "ERROR"
        return item

class SfProductPipeline(object):
    def process_item(self,item,spider):
        # return item
        if 'crawl_product_id' in item and 'crawl_product_name' in item:
            spider.spider_log.info('items log, product item')
            conn = spider.conn
            cursor = spider.cursor
            product_dict = spider.product_dict
            howbuy_product_dict = spider.howbuy_product_dict
            product_name = item['crawl_product_name']
            howbuy_id = item['crawl_product_id']
            if howbuy_id not in howbuy_product_dict:
                if product_name not in product_dict:
                    #新产品，从未采集过
                    try:
                        cursor.execute(
                            'INSERT INTO sf_product(crawl_id,name,full_name,nav_date,company_id,manager_list,start_date,trustee_bank,product_type,\
                            company_name,min_purchase_amount,min_append_amount,structured,crawl_url,crawl_comapny_id,crawl_managers_list,crawl_create_time,crawl_update_time)\
                             VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)',
                            [item['crawl_product_id'], item['crawl_product_name'], item['crawl_product_full_name'], item['nav_date'], 1008610010
                            , item['manager_list'],item['start_date'],item['trustee_bank'],item['product_type'],item['company_name']
                            ,item['min_purchase_amount'],item['min_append_amount'],item['structured'],item['crawl_url'],item['crawl_comapny_id'],item['manager_list'],item['now_time'],item['now_time']])
                        new_product_id = cursor.lastrowid
                        cursor.execute('INSERT INTO sf_product_data(id,crawl_id,subscription_fee,fixed_management_fee,ransom_fee,commission,open_date,locked_time) VALUES(%s,%s,%s,%s,%s,%s,%s,%s)',
                            [int(conn.insert_id()),item['crawl_product_id'],item['subscription_fee'],item['fixed_management_fee'], item['ransom_fee'],item['commission'], item['open_date'], item['locked_time']])
                        #self.spider_log.info('product id: ' + productData_id)
                        conn.commit()
                        spider.product_dict[product_name] = {'id':new_product_id,'name':product_name,'status':0,
                                                             'howbuy_id':howbuy_id,'howbuy_update_time': item['now_time']}
                        spider.howbuy_product_dict[howbuy_id] = {'id':new_product_id,'name':item['crawl_product_name'],
                                                                 'status':0,'howbuy_id':howbuy_id,'howbuy_update_time': item['now_time']}
                    except MySQLdb.Error, e:
                        print str(e)
                else:
                    product_info = product_dict[product_name]
                    product_id = product_info['id']
                    try:
                        #该产品已经存在，可能为手工录入，可能是第一次采集，故无匹配关系
                        cursor.execute('UPDATE sf_product set crawl_id = %s,nav_date = %s,manager_list = %s,start_date = %s,trustee_bank = %s,product_type = %s\
                         ,min_purchase_amount = %s,structured = %s,crawl_url = %s,crawl_comapny_id = %s,crawl_managers_list = %s,crawl_update_time = %s where id = %s',
                         [howbuy_id,item['nav_date'],item['manager_list'],item['start_date'],item['trustee_bank'],item['product_type'],
                         item['min_purchase_amount'],item['structured'],item['crawl_url'],item['crawl_comapny_id'],item['manager_list'],item['now_time'],product_id])
                        conn.commit()
                        spider.product_dict[product_name] = {'id': product_id,'name': product_name,'status': 0,
                                                             'howbuy_id': howbuy_id,'howbuy_update_time': item['now_time']}
                        spider.howbuy_product_dict[howbuy_id] = {'id': product_id,'name': product_name,'status': 0,
                                                                 'howbuy_id': howbuy_id,'howbuy_update_time': item['now_time']}
                    except MySQLdb.Error, e:
                        print '**************', str(e)
                pass
            else:
                if product_name not in product_dict:
                    #采集过，但是产品名字修改
                    #TODO 需讨论
                    pass
                else:
                    #完全匹配
                    product_info = product_dict[product_name]
                    product_id = product_info['id']
                    try:
                        #该产品已经存在，可能为手工录入，可能是第一次采集，故无匹配关系
                        cursor.execute('UPDATE sf_product set crawl_id = %s,nav_date = %s,manager_list = %s,start_date = %s,trustee_bank = %s,product_type = %s\
                        ,min_purchase_amount = %s,structured = %s,crawl_url = %s,crawl_comapny_id = %s,crawl_managers_list = %s,crawl_update_time = %s where id = %s',
                        [howbuy_id,item['nav_date'],item['manager_list'],item['start_date'],item['trustee_bank'],item['product_type'],
                        item['min_purchase_amount'],item['structured'],item['crawl_url'],item['crawl_comapny_id'],item['manager_list'],item['now_time'],product_id])
                        conn.commit()
                    except MySQLdb.Error, e:
                        print '**************', str(e)
                    pass
                pass
                pass
        return item

class SfCompanyPipeline(object):
    def process_item(self,item,spider):
        if 'core_manager_name' in item and 'registered_capital' in item:
            spider.spider_log.info('items log, company item step0')
            conn = spider.conn
            cursor = spider.cursor
            company_dict = spider.company_dict
            howbuy_company_dict = spider.howbuy_company_dict
            company_name = item['name']
            howbuy_id = item['crawl_id']
            spider.spider_log.info('items log, company item step5')
            if howbuy_id not in howbuy_company_dict:
                if company_name not in company_dict:
                    spider.spider_log.info('items log, step1')
                    #新公司，从未采集过
                    try:
                        cursor.execute(
                            'INSERT INTO sf_company(crawl_url,crawl_create_time,crawl_update_time,crawl_id,full_name,name,core_manager_name,rep_product_name,\
                            icp,establishment_date,registered_capital,region,update_time) VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)',
                            [item['crawl_url'],item['crawl_create_time'],item['crawl_update_time'],item['crawl_id'],item['full_name'],
                            item['name'], item['core_manager_name'], item['rep_product_name'],item['icp'],item['establishment_date'],
                            item['registered_capital'],item['region'],item['update_time']])
                        spider.spider_log.info(int(cursor.lastrowid))
                        lastid = int(cursor.lastrowid)
                        cursor.execute('INSERT INTO sf_company_data(id) VALUES(%s)',[lastid])

                        #update company dict
                        update_company_info = {
                            'id': lastid,
                            'name': item['name'],
                            'status': 0,
                            'howbuy_id': howbuy_id,
                            'howbuy_update_time': item['crawl_update_time']
                        }
                        conn.commit()
                        spider.spider_log.info(update_company_info)
                        spider.company_dict[company_name] = update_company_info
                        spider.howbuy_company_dict[howbuy_id] = update_company_info
                        #update sf_product companye_id
                        sfproduct_ids_sql = 'SELECT id FROM `sf_product` where crawl_comapny_id=%s'
                        cursor.execute (sfproduct_ids_sql,[howbuy_id])
                        sfproductIds = cursor.fetchall()
                        for sfproduct_id in sfproductIds:
                            update_product_company_id_sql = 'UPDATE sf_product SET company_id =%s,update_time=%s WHERE id = %s'
                            cursor.execute(update_product_company_id_sql,[lastid,item['update_time'],sfproduct_id])
                            conn.commit()

                        # update sf_manager companye_id
                        sfmanager_ids_sql = 'SELECT id FROM `sf_manager` where crawl_comapny_id=%s'
                        cursor.execute(sfmanager_ids_sql, [howbuy_id])
                        sfmanagerIds = cursor.fetchall()
                        for sfmanager_id in sfmanagerIds:
                            update_manager_company_id_sql = 'UPDATE sf_manager SET company_id =%s,update_time=%s WHERE id = %s'
                            cursor.execute(update_manager_company_id_sql, [lastid, item['update_time'], sfmanager_id])
                            conn.commit()
                    except MySQLdb.Error, e:
                        print '**************', str(e)
                else:
                    company_info = company_dict[company_name]
                    company_id = company_info['id']
                    spider.spider_log.info('items log, step2')
                    try:
                        #该公司已经存在，可能为手工录入，可能是第一次采集，故无匹配关系
                        cursor.execute('UPDATE sf_company set crawl_id=%s,full_name=%s,crawl_update_time =%s, crawl_url=%s, core_manager_name=%s,\
                        rep_product_name=%s, icp=%s, establishment_date=%s, registered_capital=%s, region=%s ,update_time=%s where id = %s',[howbuy_id ,
                        item['full_name'],item['crawl_update_time'],item['crawl_url'],item['core_manager_name'],item['rep_product_name'],item['icp'],
                        item['establishment_date'],item['registered_capital'],item['region'],item['update_time'],company_id])
                        conn.commit()
                        # update company dict
                        update_company_info = {
                            'id': company_id,
                            'name': item['name'],
                            'status': 0,
                            'howbuy_id': howbuy_id,
                            'howbuy_update_time': item['crawl_update_time']
                        }
                        conn.commit()
                        spider.company_dict[company_name] = update_company_info
                        spider.howbuy_company_dict[howbuy_id] = update_company_info
                    except MySQLdb.Error, e:
                        print '**************', str(e)
                pass
            else:
                if company_name not in company_dict:
                    spider.spider_log.info('items log, step3')
                    #采集过，但是产品名字修改
                    #TODO 需讨论
                    pass
                else:
                    spider.spider_log.info('items log, step4')
                    #完全匹配
                    company_info = company_dict[company_name]
                    company_id = company_info['id']
                    try:
                        #该产品已经存在，可能为手工录入，可能是第一次采集，故无匹配关系
                        cursor.execute('UPDATE sf_company set crawl_id = %s,update_time=%s where id = %s',[howbuy_id ,item['update_time'], company_id])
                        conn.commit()
                    except MySQLdb.Error, e:
                        print '**************', str(e)
                    pass
        return item


class NavPipeline(object):
    def process_item(self, item, spider):
        if 'nav_item_date' in item:
            spider.spider_log.info('nav item is inserting')
            cursor = spider.cursor
            conn = spider.conn
            sql = 'insert into sf_nav(product_id,crawl_id,nav_date,added_nav,nav,growth_rate,create_time,update_time,crawl_time,crawl_url) values(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)'
            try:
                cursor.executemany(sql, item['nav_item_date'])
                conn.commit()
            except MySQLdb.Error, e:
                spider.spider_log.error('插入失败的produt_id: '+str(item))
            pass
            # pass
        return item
    def _calculate_statistics(self,product_id,cursor):
        nav_select_sql = 'SELECT product_id,nav_date,nav,added_nav,growth_rate,create_time,update_time FROM sf_nav WHERE \
                         product_id = %s ORDER BY nav_date Desc'
        cursor.execute(nav_select_sql,[product_id])
        sf_navs = cursor.fetchall()
        sf_nav_dict = {}
        sf_latest_date = ''
        sf_first_date = ''
        if not all(sf_navs):
            date_index = 0
            sf_latest_date = str(sf_navs[0][1])
            target_date = {}
            o_latest_date = datetime.datetime.strptime(sf_latest_date, '%Y-%m-%d').date()
            for i in range(1,15):
                target_date = get_date_by_interval(sf_latest_date,i)
                target_date[i] = {
                    'target_date' : target_date,
                    'target_nav_date' : sf_latest_date,
                    'offset' : abs((o_latest_date - target_date).days)
                }

            for nav_info in sf_navs:
                nav_date = str(nav_info[1])
                nav = nav_info[2]
                added_nav = nav_info[3]
                sf_nav_dict[nav_date] = {'nav_date':nav_date,'nav':nav,'added_nav':added_nav}
                o_nav_date = datetime.datetime.strptime(nav_date, '%Y-%m-%d').date()
                for i in range(1,15):
                    target_date = target_date[i].target_date
                    offset = abs((o_nav_date - target_date).days)
                    if offset < target_date[i]:
                        target_date[i]['target_nav_date'] = nav_date
                        target_date[i]['offset'] = offset
                pass
            sf_first_date = max(sf_nav_dict.keys())
            o_sf_first_date = datetime.datetime.strptime(sf_first_date, '%Y-%m-%d').date()
            max_offset = (o_sf_first_date - o_latest_date).days
            for i in range(1,15):
                o_target_info = target_date[i]
                if o_target_info['offset'] > max_offset:
                    target_date[i] = None
                pass
            from_start_date = sf_first_date
            if 1 in target_date:
                last_month_date = target_date[1]['target_nav_date']
            if 2 in target_date:
                last_3month_date = target_date[2]['target_nav_date']
            if 3 in target_date:
                last_6_month_date = target_date[3]['target_nav_date']
            if 4 in target_date:
                this_year_date = target_date[4]['target_nav_date']
            if 5 in target_date:
                last_year_date = target_date[5]['target_nav_date']
            if 6 in target_date:
                last_2year_date = target_date[6]['target_nav_date']
            if 7 in target_date:
                last_3_year_date = target_date[7]['target_nav_date']
            if 8 in target_date:
                last_4_year_date = target_date[8]['target_nav_date']
            if 9 in target_date:
                last_5_year_date = target_date[9]['target_nav_date']
            if 10 in target_date:
                return_year_date = target_date[10]['target_nav_date']
            if 11 in target_date:
                return_2year_date = target_date[11]['target_nav_date']
            if 12 in target_date:
                return_3year_date = target_date[12]['target_nav_date']
            if 13 in target_date:
                return_4year_date = target_date[13]['target_nav_date']
            if 14 in target_date:
                return_5_year_date = target_date[14]['target_nav_date']
        pass

class SfManagerPipeline(object):
    def process_item(self,item,spider):        
        if 'typical_product_id' in item:
            conn = spider.conn
            cursor = spider.cursor
            manager_dict = spider.manager_dict
            howbuy_manager_dict = spider.howbuy_manager_dict
            manager_name = item['manager_name']
            company_name = item['company_name']
            manager_company = item['manager_name']+'-'+item['company_name']
            howbuy_id = item['crawl_id']
            if manager_company not in manager_dict:
                #新经理，从未采集过
                try:
                    cursor.execute(
                        'INSERT INTO sf_manager(name,company_name,profile,background,invest_year,manage_product_num,\
                        crawl_id,crawl_company_id,crawl_product_id,\
                        crawl_create_time, create_time, update_time) VALUES(%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s,%s)',
                        [item['manager_name'],item['company_name'],item['profile'],item['background'],
                         item['invest_year'], item['manage_product_num'], item['crawl_id'], item['company_id'],
                         item['typical_product_id'],item['now_time'], item['create_time'], item['update_time']])
                    conn.commit()
                except MySQLdb.Error, e:
                    print '**************', str(e)
            else:
                manager_info = manager_dict[manager_company]
                manager_id = manager_info['id']
                company_name = manager_info['company_name']
                try:
                    #该经理已经存在，可能为手工录入，可能是第一次采集，故无匹配关系
                    cursor.execute('UPDATE sf_manager set profile=%s, background =%s, invest_year=%s, manage_product_num=%s,\
                    crawl_id=%s, crawl_company_id=%s, crawl_product_id=%s, crawl_update_time=%s, update_time = %s \
                    where id = %s and company_name = %s',[item['profile'],item['background'],item['invest_year'],
                                                          item['manage_product_num'], item['crawl_id'], item['company_id'],
                                                          item['typical_product_id'],item['now_time'],item['update_time'],
                                                          manager_id,company_name])
                    conn.commit()
                except MySQLdb.Error, e:
                    print '**************', str(e)
        return item
class MonthReturnPipeline(object):
    def process_item(self, item, spider):
        with open("g.txt", 'a') as f:
            f.write('item,' + str(item) + "\r\n")
        if 'month_return' in item:
            conn = spider.conn
            cursor = spider.cursor
            sql = 'insert into sf_return_drawdown(`product_id`,`type`,`year_month`,`nav`,`added_nav`,`value`,`hs300`,`create_time`,`update_time`) values(%s,%s,%s,%s,%s,%s,%s,%s,%s)'
            try:
                cursor.executemany(sql, item['month_return'])
                conn.commit()
            except MySQLdb.Error, e:
                spider.spider_log.info('插入失败的produt_id: ' + str(item['month_return']))
            pass

#沪深300
class HsPipeline(object):
    def process_item(self, item, spider):
        if 'hs_return' in item:
            conn = spider.conn
            cursor = spider.cursor
            sql = 'insert into sf_hs_index(`index_type`,`index_date`,`hs_index`,`growth_rate`,`total_growth_rate`,`create_time`,`update_time`) values(%s,%s,%s,%s,%s,%s,%s)'
            try:
                cursor.executemany(sql, item['hs_return'])
                conn.commit()
                print "INSERT was successful"
            except MySQLdb.Error, e:
                spider.spider_log.info('插入失败的produt_id: ' + str(item['hs_return']))
            pass
